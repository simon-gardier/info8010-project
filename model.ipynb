{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1485588",
   "metadata": {},
   "source": [
    "# **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cba002",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wandb matplotlib torch torchviz torchvision torchsummary torchviz weave nbformat netron onnx --quiet\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data as data\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import datasets, transforms, utils\n",
    "import wandb\n",
    "import netron\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7d184",
   "metadata": {},
   "source": [
    "# **Util Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    plt.imshow(transforms.functional.to_pil_image(img))\n",
    "    plt.show()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, labels\n",
    "\n",
    "def show_image_with_labels(image, labels, class_names=None):\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    h, w, _ = image_np.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    for label in labels:\n",
    "        class_id, x_center, y_center, bw, bh = label.tolist()\n",
    "        x = (x_center - bw / 2) * w\n",
    "        y = (y_center - bh / 2) * h\n",
    "        box_w = bw * w\n",
    "        box_h = bh * h\n",
    "        rect = patches.Rectangle((x, y), box_w, box_h, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        if class_names:\n",
    "            class_text = class_names[int(class_id)]\n",
    "        else:\n",
    "            class_text = str(int(class_id))\n",
    "        ax.text(x, y - 5, class_text, color='white', fontsize=12,bbox=dict(facecolor='red', alpha=0.5, pad=2))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# classes_types = {\n",
    "#     0: 'cow',\n",
    "#     1: 'duck',\n",
    "#     2: 'flower',\n",
    "#     3: 'people',\n",
    "#     4: 'pig',\n",
    "#     5: 'rabbit',\n",
    "#     6: 'sheep',\n",
    "# }\n",
    "classes_types = {\n",
    "    0: 'coal_ore',\n",
    "    1: 'diamond_ore',\n",
    "    2: 'emerald_ore',\n",
    "    3: 'gold_ore',\n",
    "    4: 'iron_ore',\n",
    "    5: 'lapis_ore',\n",
    "    6: 'redstone_ore',\n",
    "}\n",
    "classes_number = len(classes_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57471f90",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afac7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from os import path\n",
    "\n",
    "class MinecraftV1(Dataset):\n",
    "    def __init__(self, root, train=True, valid=False, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            self.data_path = path.join(root, 'train')\n",
    "        elif valid:\n",
    "            self.data_path = path.join(root, 'valid')\n",
    "        else:\n",
    "            self.data_path = path.join(root, 'test')\n",
    "        \n",
    "        self.images_path = path.join(self.data_path, 'images')\n",
    "        self.labels_path = path.join(self.data_path, 'labels')\n",
    "        self.data_images = []\n",
    "        self.data_labels = []\n",
    "        image_files = os.listdir(self.images_path)\n",
    "        label_files = os.listdir(self.labels_path)\n",
    "        for image_file in image_files:\n",
    "            image_path = path.join(self.images_path, image_file)\n",
    "            self.data_images.append(image_path)\n",
    "\n",
    "        for label_file in label_files:\n",
    "            label_path = path.join(self.labels_path,label_file)\n",
    "            self.data_labels.append(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_images[idx]\n",
    "        label_path = self.data_labels[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        with open(label_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        labels = [list(map(float, line.strip().split())) for line in lines]\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return image, labels\n",
    "    \n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mc_train = MinecraftV1(root=os.getcwd(), transform=basic_transform)\n",
    "mc_test = MinecraftV1(root=os.getcwd(), train=False, transform=basic_transform)\n",
    "mc_valid = MinecraftV1(root=os.getcwd(), train=False,valid=True, transform=basic_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86a5b4",
   "metadata": {},
   "source": [
    "# **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(mc_train, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "images, labels = next(iter(trainloader))\n",
    "for i in range(4):\n",
    "    show_image_with_labels(images[i], labels[i], class_names=None)\n",
    "\n",
    "validloader = data.DataLoader(mc_valid, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c37134",
   "metadata": {},
   "source": [
    "# **Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4727c7c",
   "metadata": {},
   "source": [
    "# **Yolo V5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c45811",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Basic Conv-BN-SiLU block\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=None):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.SiLU()\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.batch_norm(self.conv(x)))\n",
    "\n",
    "# Bottleneck for C3\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True, expansion=0.5):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = Conv(in_channels, hidden_channels, 1)\n",
    "        self.conv2 = Conv(hidden_channels, out_channels, 3)\n",
    "        self.use_shortcut = shortcut and in_channels == out_channels\n",
    "    def forward(self, x):\n",
    "        out = self.conv2(self.conv1(x))\n",
    "        return x + out if self.use_shortcut else out\n",
    "\n",
    "# CSP C3 module\n",
    "class C3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=1, shortcut=True, expansion=0.5):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = Conv(in_channels, hidden_channels, 1)\n",
    "        self.conv2 = Conv(in_channels, hidden_channels, 1)\n",
    "        self.bottlenecks = nn.Sequential(\n",
    "            *[Bottleneck(hidden_channels, hidden_channels, shortcut, expansion) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.conv3 = Conv(2 * hidden_channels, out_channels, 1)\n",
    "    def forward(self, x):\n",
    "        return self.conv3(torch.cat((self.bottlenecks(self.conv1(x)), self.conv2(x)), dim=1))\n",
    "\n",
    "# SPPF module\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.conv1 = Conv(in_channels, hidden_channels, 1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
    "        self.conv2 = Conv(hidden_channels * 4, out_channels, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.maxpool(x)\n",
    "        y2 = self.maxpool(y1)\n",
    "        y3 = self.maxpool(y2)\n",
    "        return self.conv2(torch.cat([x, y1, y2, y3], dim=1))\n",
    "\n",
    "# Backbone CSPDarknet\n",
    "class CSPDarknet(nn.Module):\n",
    "    def __init__(self, width_mult=1.0):\n",
    "        super().__init__()\n",
    "        width = lambda c: int(c * width_mult)\n",
    "        self.stem = Conv(3, width(64), kernel_size=6, stride=2, padding=2)\n",
    "        self.stage2 = nn.Sequential(\n",
    "            Conv(width(64), width(128), 3, 2),\n",
    "            C3(width(128), width(128), num_blocks=3)\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            Conv(width(128), width(256), 3, 2),\n",
    "            C3(width(256), width(256), num_blocks=6)\n",
    "        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "            Conv(width(256), width(512), 3, 2),\n",
    "            C3(width(512), width(512), num_blocks=9)\n",
    "        )\n",
    "        self.stage5 = nn.Sequential(\n",
    "            Conv(width(512), width(1024), 3, 2),\n",
    "            C3(width(1024), width(1024), num_blocks=3),\n",
    "            SPPF(width(1024), width(1024), kernel_size=5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        feature2 = self.stage2(x)\n",
    "        feature3 = self.stage3(feature2)\n",
    "        feature4 = self.stage4(feature3)\n",
    "        feature5 = self.stage5(feature4)\n",
    "        return feature3, feature4, feature5  # P3/8, P4/16, P5/32\n",
    "\n",
    "# PANet neck\n",
    "class PANet(nn.Module):\n",
    "    def __init__(self, width_mult=1.0):\n",
    "        super().__init__()\n",
    "        width = lambda c: int(c * width_mult)\n",
    "        # top-down pathway\n",
    "        self.reduce_conv_p5     = Conv(width(1024), width(512), 1)\n",
    "        self.reduce_conv_p4     = Conv(width(512), width(512), 1)\n",
    "        self.c3_topdown_p4      = C3(width(1024), width(512), num_blocks=1)\n",
    "        self.reduce_conv_p4_td  = Conv(width(512), width(256), 1)\n",
    "        self.reduce_conv_p3     = Conv(width(256), width(256), 1)\n",
    "        self.c3_topdown_p3      = C3(width(512), width(256), num_blocks=1)\n",
    "        # bottom-up pathway\n",
    "        self.downsample_conv_p3 = Conv(width(256), width(256), 3, 2)\n",
    "        self.c3_bottomup_p4     = C3(width(256)+width(512), width(512), num_blocks=1)\n",
    "        self.downsample_conv_p4 = Conv(width(512), width(512), 3, 2)\n",
    "        self.c3_bottomup_p5     = C3(width(512)+width(512), width(1024), num_blocks=1)\n",
    "        self.upsample           = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x3, x4, x5):\n",
    "        # top-down\n",
    "        p5 = self.reduce_conv_p5(x5)\n",
    "        p5_up = self.upsample(p5)\n",
    "        p4 = self.reduce_conv_p4(x4)\n",
    "        p4_td = self.c3_topdown_p4(torch.cat([p4, p5_up], 1))\n",
    "\n",
    "        p4_td_reduced = self.reduce_conv_p4_td(p4_td)\n",
    "        p4_up = self.upsample(p4_td_reduced)\n",
    "        p3 = self.reduce_conv_p3(x3)\n",
    "        p3_td = self.c3_topdown_p3(torch.cat([p3, p4_up], 1))\n",
    "\n",
    "        # bottom-up\n",
    "        p3_down = self.downsample_conv_p3(p3_td)\n",
    "        p4_bu = self.c3_bottomup_p4(torch.cat([p3_down, p4_td], 1))\n",
    "\n",
    "        p4_down = self.downsample_conv_p4(p4_bu)\n",
    "        p5_bu = self.c3_bottomup_p5(torch.cat([p4_down, p5], 1))\n",
    "        return p3_td, p4_bu, p5_bu\n",
    "\n",
    "# Detect head\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes=7, anchors=(), channels=()):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_outputs = num_classes + 5\n",
    "        self.num_layers = len(anchors)\n",
    "        self.num_anchors = len(anchors[0]) // 2\n",
    "        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.num_layers, -1, 2))\n",
    "        self.detect_convs = nn.ModuleList([nn.Conv2d(ch, self.num_outputs * self.num_anchors, 1) for ch in channels])\n",
    "    def forward(self, features):\n",
    "        outputs = []\n",
    "        for i in range(self.num_layers):\n",
    "            pred = self.detect_convs[i](features[i])\n",
    "            batch_size, _, height, width = pred.shape\n",
    "            pred = pred.view(batch_size, self.num_anchors, self.num_outputs, height, width).permute(0, 1, 3, 4, 2).contiguous()\n",
    "            outputs.append(pred)\n",
    "        return outputs\n",
    "\n",
    "# Full YOLOv5 model\n",
    "class YoloV5m(nn.Module):\n",
    "    def __init__(self, num_classes=7, anchors=None):\n",
    "        super().__init__()\n",
    "        if anchors is None:\n",
    "            anchors = [\n",
    "                [10, 13, 16, 30, 33, 23],\n",
    "                [30, 61, 62, 45, 59, 119],\n",
    "                [116, 90, 156, 198, 373, 326]\n",
    "            ]\n",
    "        self.backbone = CSPDarknet()\n",
    "        self.neck = PANet()\n",
    "        self.detect = Detect(num_classes, anchors, channels=[256, 512, 1024])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_p3, feature_p4, feature_p5 = self.backbone(x)\n",
    "        neck_p3, neck_p4, neck_p5 = self.neck(feature_p3, feature_p4, feature_p5)\n",
    "        return self.detect([neck_p3, neck_p4, neck_p5])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YoloV5m(num_classes=classes_number).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Netron visualization\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "torch.onnx.export(model, x, \"yolov5.onnx\", opset_version=12)\n",
    "netron.start(\"yolov5.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e450c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(labels, anchors, grid_sizes, num_classes, device):\n",
    "    num_scales = 3\n",
    "    targets = []\n",
    "    for scale_idx in range(num_scales):\n",
    "        grid_size = grid_sizes[scale_idx]\n",
    "        stride = 640 / grid_size\n",
    "        current_anchors = anchors[scale_idx]\n",
    "        target = torch.zeros((len(labels), len(current_anchors), grid_size, grid_size, 6), device=device)\n",
    "        \n",
    "        for batch_idx, label in enumerate(labels):\n",
    "            if label.numel() == 0:\n",
    "                continue\n",
    "            gt_boxes = label[:, 1:5] * 640\n",
    "            gt_classes = label[:, 0].long()\n",
    "            x_center = gt_boxes[:, 0]\n",
    "            y_center = gt_boxes[:, 1]\n",
    "            w = gt_boxes[:, 2]\n",
    "            h = gt_boxes[:, 3]\n",
    "            \n",
    "            for box_idx in range(gt_boxes.shape[0]):\n",
    "                x = x_center[box_idx]\n",
    "                y = y_center[box_idx]\n",
    "                box_w = w[box_idx]\n",
    "                box_h = h[box_idx]\n",
    "                \n",
    "                # Find best anchor\n",
    "                current_anchors_tensor = torch.tensor(current_anchors, device=device)\n",
    "                ratios = torch.cat([box_w / current_anchors_tensor[:, 0],\n",
    "                                  box_h / current_anchors_tensor[:, 1]]).view(2, -1).T\n",
    "                max_ratios = torch.max(ratios, 1/ratios).max(dim=1)[0]\n",
    "                best_anchor = max_ratios.argmin()\n",
    "                \n",
    "                # Grid cell\n",
    "                i = (x / stride).floor().long()\n",
    "                j = (y / stride).floor().long()\n",
    "                if i >= grid_size or j >= grid_size or i < 0 or j < 0:\n",
    "                    continue\n",
    "                \n",
    "                # Target values\n",
    "                tx = (x / stride) - i.float()\n",
    "                ty = (y / stride) - j.float()\n",
    "                tw = torch.log(box_w / current_anchors_tensor[best_anchor, 0] + 1e-16)\n",
    "                th = torch.log(box_h / current_anchors_tensor[best_anchor, 1] + 1e-16)\n",
    "                \n",
    "                target[batch_idx, best_anchor, j, i, 0] = 1.0  # obj\n",
    "                target[batch_idx, best_anchor, j, i, 1] = tx\n",
    "                target[batch_idx, best_anchor, j, i, 2] = ty\n",
    "                target[batch_idx, best_anchor, j, i, 3] = tw\n",
    "                target[batch_idx, best_anchor, j, i, 4] = th\n",
    "                target[batch_idx, best_anchor, j, i, 5] = gt_classes[box_idx]\n",
    "                \n",
    "        targets.append(target)\n",
    "    return targets\n",
    "\n",
    "def compute_loss(outputs, targets):\n",
    "    total_loss = 0.0\n",
    "    obj_loss = 0.0\n",
    "    box_loss = 0.0\n",
    "    cls_loss = 0.0\n",
    "    \n",
    "    for y_pred, target in zip(outputs, targets):\n",
    "        # Predicted components\n",
    "        pred_tx = y_pred[..., 0]\n",
    "        pred_ty = y_pred[..., 1]\n",
    "        pred_tw = y_pred[..., 2]\n",
    "        pred_th = y_pred[..., 3]\n",
    "        pred_obj = y_pred[..., 4]\n",
    "        pred_cls = y_pred[..., 5:]\n",
    "        \n",
    "        # Target components\n",
    "        target_obj = target[..., 0]\n",
    "        target_tx = target[..., 1]\n",
    "        target_ty = target[..., 2]\n",
    "        target_tw = target[..., 3]\n",
    "        target_th = target[..., 4]\n",
    "        target_cls = target[..., 5].long()\n",
    "        \n",
    "        # Objectness loss\n",
    "        obj_mask = target_obj.bool()\n",
    "        obj_loss += F.binary_cross_entropy_with_logits(pred_obj, target_obj)\n",
    "        \n",
    "        # Box loss (only where objects exist)\n",
    "        if obj_mask.sum():\n",
    "            box_loss += F.mse_loss(pred_tx[obj_mask], target_tx[obj_mask]) + \\\n",
    "                       F.mse_loss(pred_ty[obj_mask], target_ty[obj_mask]) + \\\n",
    "                       F.mse_loss(pred_tw[obj_mask], target_tw[obj_mask]) + \\\n",
    "                       F.mse_loss(pred_th[obj_mask], target_th[obj_mask])\n",
    "        \n",
    "        # Class loss\n",
    "        if obj_mask.sum():\n",
    "            cls_loss += F.cross_entropy(pred_cls[obj_mask], target_cls[obj_mask])\n",
    "    \n",
    "    total_loss = obj_loss + box_loss + cls_loss\n",
    "    return total_loss, obj_loss, box_loss, cls_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169e727",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73677f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"s-gardier-work\",\n",
    "    project=\"yolov5\",\n",
    "    config={\n",
    "        \"architecture\": \"YOLOv5\",\n",
    "        \"dataset\": \"https://universe.roboflow.com/yolo-minecraft/minecraft-ogpjp/dataset/10\",\n",
    "        \"epochs\": num_epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "        \n",
    "        # Generate targets\n",
    "        anchors = model.anchors.cpu().numpy().tolist()\n",
    "        grid_sizes = [80, 40, 20]\n",
    "        targets = build_targets(labels, anchors, grid_sizes, classes_number, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, obj_loss, box_loss, cls_loss = compute_loss(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}\")\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"yolov5_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Model saved at epoch {epoch + 1}\")\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(validloader):\n",
    "        images = images.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "        \n",
    "        # Generate targets\n",
    "        anchors = model.anchors.cpu().numpy().tolist()\n",
    "        grid_sizes = [80, 40, 20]\n",
    "        targets = build_targets(labels, anchors, grid_sizes, classes_number, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, obj_loss, box_loss, cls_loss = compute_loss(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        val_running_loss += total_loss.item()\n",
    "    print(f\"Validation Loss: {val_running_loss / len(validloader):.4f}\")\n",
    "    run.log({\"epoch\": (epoch + 1)/num_epochs, \"training_loss\": running_loss / len(trainloader), \"validation_loss\": val_running_loss / len(validloader)})\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bc07d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path=\"models/yolov5_epoch_20.pth\"):\n",
    "    \"\"\"Load a model from a .pth file.\"\"\"\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def decode_predictions(outputs, confidence_thresh=0.1, iou_thresh=0.1):\n",
    "    \"\"\"Convert model outputs to usable bounding boxes\"\"\"\n",
    "    all_boxes = []\n",
    "    anchors = model.anchors.cpu().numpy()\n",
    "    strides = [8, 16, 32]  # For 640x640 input\n",
    "    \n",
    "    for scale_idx, output in enumerate(outputs):\n",
    "        # Convert to numpy and get dimensions\n",
    "        output = output.sigmoid().cpu().detach().numpy()\n",
    "        bs, num_anchors, h, w, _ = output.shape\n",
    "        \n",
    "        # Convert dimensions to integers\n",
    "        h = int(h)\n",
    "        w = int(w)\n",
    "        \n",
    "        # Get parameters for this scale\n",
    "        stride = strides[scale_idx]\n",
    "        anchor = anchors[scale_idx]\n",
    "        \n",
    "        # Create grid\n",
    "        grid_y, grid_x = np.mgrid[:h, :w]\n",
    "\n",
    "        # Reshape output for vectorized operations\n",
    "        output = output.reshape(bs, num_anchors, h, w, -1)\n",
    "        \n",
    "        # Decode predictions using vectorized operations\n",
    "        tx = output[..., 0]\n",
    "        ty = output[..., 1]\n",
    "        tw = output[..., 2]\n",
    "        th = output[..., 3]\n",
    "        obj = output[..., 4]\n",
    "        cls_probs = output[..., 5:]\n",
    "        \n",
    "        # Calculate absolute coordinates\n",
    "        x = (grid_x + tx) * stride\n",
    "        y = (grid_y + ty) * stride\n",
    "        w = anchor[:, 0].reshape(1, -1, 1, 1) * np.exp(tw)\n",
    "        h = anchor[:, 1].reshape(1, -1, 1, 1) * np.exp(th)\n",
    "        \n",
    "        # Calculate class confidence\n",
    "        class_ids = np.argmax(cls_probs, axis=-1)\n",
    "        class_conf = np.take_along_axis(cls_probs, class_ids[..., None], axis=-1).squeeze(-1)\n",
    "        confidence = obj * class_conf\n",
    "\n",
    "        # Filter by confidence threshold\n",
    "        mask = confidence > confidence_thresh\n",
    "        for batch_idx in range(bs):\n",
    "            batch_mask = mask[batch_idx]\n",
    "            batch_boxes = np.stack([\n",
    "                x[batch_idx][batch_mask] - w[batch_idx][batch_mask]/2,\n",
    "                y[batch_idx][batch_mask] - h[batch_idx][batch_mask]/2,\n",
    "                x[batch_idx][batch_mask] + w[batch_idx][batch_mask]/2,\n",
    "                y[batch_idx][batch_mask] + h[batch_idx][batch_mask]/2,\n",
    "                confidence[batch_idx][batch_mask],\n",
    "                class_ids[batch_idx][batch_mask]\n",
    "            ], axis=-1)\n",
    "            \n",
    "            if batch_boxes.size > 0:\n",
    "                all_boxes.extend(batch_boxes.tolist())\n",
    "\n",
    "    # Non-Maximum Suppression\n",
    "    if not all_boxes:\n",
    "        return []\n",
    "\n",
    "    boxes = np.array(all_boxes)\n",
    "    x1, y1, x2, y2, scores, class_ids = boxes.T\n",
    "\n",
    "    # Calculate areas and sort\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        \n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        intersection = w * h\n",
    "        \n",
    "        iou = intersection / (areas[i] + areas[order[1:]] - intersection)\n",
    "        \n",
    "        # Filter boxes\n",
    "        inds = np.where(iou <= iou_thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return boxes[keep].tolist()\n",
    "\n",
    "def test_single_image(model, dataset, index=0):\n",
    "    # Get image and labels\n",
    "    image, true_labels = dataset[index]\n",
    "    true_labels = true_labels.cpu().numpy()\n",
    "    \n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image.unsqueeze(0).to(device))\n",
    "    \n",
    "    # Decode predictions\n",
    "    pred_boxes = decode_predictions(outputs)\n",
    "    \n",
    "    # Convert true labels to box format\n",
    "    true_boxes = []\n",
    "    img_w, img_h = 640, 640  # Our image size\n",
    "    for label in true_labels:\n",
    "        class_id, xc, yc, bw, bh = label\n",
    "        x = (xc - bw/2) * img_w\n",
    "        y = (yc - bh/2) * img_h\n",
    "        w = bw * img_w\n",
    "        h = bh * img_h\n",
    "        true_boxes.append([x, y, x+w, y+h, 1.0, class_id])\n",
    "    \n",
    "    # Visualize\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image_np)\n",
    "    \n",
    "    # Draw true boxes (green)\n",
    "    for box in true_boxes:\n",
    "        x1, y1, x2, y2, _, class_id = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='lime', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, classes_types[int(class_id)],\n",
    "                color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='lime', alpha=0.8, pad=1))\n",
    "    \n",
    "    # Draw predicted boxes (red)\n",
    "    for box in pred_boxes:\n",
    "        x1, y1, x2, y2, conf, class_id = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, f\"{classes_types[int(class_id)]} {conf:.2f}\",\n",
    "                color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='red', alpha=1, pad=1))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test on first validation image\n",
    "model_from_disk = load_model(model, \"models/yolov5_epoch_20.pth\")\n",
    "test_single_image(model_from_disk, mc_valid, index=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info8010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

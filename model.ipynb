{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1485588",
   "metadata": {},
   "source": [
    "# **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cba002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as data\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7d184",
   "metadata": {},
   "source": [
    "# **Util Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    plt.imshow(transforms.functional.to_pil_image(img))\n",
    "    plt.show()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, labels\n",
    "\n",
    "def show_image_with_labels(image, labels, class_names=None):\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    h, w, _ = image_np.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    for label in labels:\n",
    "        class_id, x_center, y_center, bw, bh = label.tolist()\n",
    "        x = (x_center - bw / 2) * w\n",
    "        y = (y_center - bh / 2) * h\n",
    "        box_w = bw * w\n",
    "        box_h = bh * h\n",
    "        rect = patches.Rectangle((x, y), box_w, box_h, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        if class_names:\n",
    "            class_text = class_names[int(class_id)]\n",
    "        else:\n",
    "            class_text = str(int(class_id))\n",
    "        ax.text(x, y - 5, class_text, color='white', fontsize=12,bbox=dict(facecolor='red', alpha=0.5, pad=2))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "classes_types = {\n",
    "    0: 'cow',\n",
    "    1: 'duck',\n",
    "    2: 'flower',\n",
    "    3: 'people',\n",
    "    4: 'pig',\n",
    "    5: 'rabbit',\n",
    "    6: 'sheep',\n",
    "}\n",
    "classes_number = len(classes_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57471f90",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afac7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from os import path\n",
    "\n",
    "class MinecraftV1(Dataset):\n",
    "    def __init__(self, root, train=True, valid=False, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            self.data_path = path.join(root, 'train')\n",
    "        elif valid:\n",
    "            self.data_path = path.join(root, 'valid')\n",
    "        else:\n",
    "            self.data_path = path.join(root, 'test')\n",
    "        \n",
    "        self.images_path = path.join(self.data_path, 'images')\n",
    "        self.labels_path = path.join(self.data_path, 'labels')\n",
    "        self.data_images = []\n",
    "        self.data_labels = []\n",
    "        image_files = os.listdir(self.images_path)\n",
    "        label_files = os.listdir(self.labels_path)\n",
    "        for image_file in image_files:\n",
    "            image_path = path.join(self.images_path, image_file)\n",
    "            self.data_images.append(image_path)\n",
    "\n",
    "        for label_file in label_files:\n",
    "            label_path = path.join(self.labels_path,label_file)\n",
    "            self.data_labels.append(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_images[idx]\n",
    "        label_path = self.data_labels[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        with open(label_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        labels = [list(map(float, line.strip().split())) for line in lines]\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return image, labels\n",
    "    \n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mc_train = MinecraftV1(root=os.getcwd(), transform=basic_transform)\n",
    "mc_test = MinecraftV1(root=os.getcwd(), train=False, transform=basic_transform)\n",
    "mc_valid = MinecraftV1(root=os.getcwd(), train=False,valid=True, transform=basic_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86a5b4",
   "metadata": {},
   "source": [
    "# **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(mc_train, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "images, labels = next(iter(trainloader))\n",
    "for i in range(4):\n",
    "    show_image_with_labels(images[i], labels[i], class_names=None)\n",
    "\n",
    "validloader = data.DataLoader(mc_valid, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c37134",
   "metadata": {},
   "source": [
    "# **Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4727c7c",
   "metadata": {},
   "source": [
    "# **Yolo V5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBS (Convolutional + BatchNorm + SiLU)\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Focus block for improved small details detection\n",
    "class FocusBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size =3, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_channels * 4, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([x[:, :, ::2, ::2],\n",
    "                       x[:, :, 1::2, ::2],\n",
    "                       x[:, :, ::2, 1::2],\n",
    "                       x[:, :, 1::2, 1::2]], dim=1)\n",
    "        return self.conv(x)\n",
    "    \n",
    "# CSPNet\n",
    "class CSP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,num_blocks):\n",
    "        super().__init__()\n",
    "        hidden_channels = out_channels // 2\n",
    "        self.conv1 = ConvBlock(in_channels, hidden_channels,1)\n",
    "        self.conv2 = ConvBlock(in_channels, hidden_channels,1 )\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            ConvBlock(hidden_channels, hidden_channels, 3)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.conv3 = ConvBlock(2*hidden_channels, out_channels,1)\n",
    "    def forward(self,x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x1 = self.blocks(x1)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.conv3(x)\n",
    "\n",
    "# Spatial Pyramid Pooling Fast\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k = 5):\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.conv1 = ConvBlock(in_channels, hidden_channels,1 )\n",
    "        self.conv2 = ConvBlock(hidden_channels * 4, out_channels,1)\n",
    "        self.pools = nn.ModuleList([nn.MaxPool2d(kernel_size=k, stride=1, padding=k//2) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return self.conv2(torch.cat([x] + [pool(x) for pool in self.pools], 1))\n",
    "\n",
    "# Backbone\n",
    "class DarkNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = FocusBlock(3, 32, 3, 1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(32, 64, 3, 2),\n",
    "            CSP(64, 64,1)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(64, 128, 3, 2),\n",
    "            CSP(128, 128,2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(128, 256, 3, 2),\n",
    "            CSP(256, 256,3)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ConvBlock(256, 512, 3, 2),\n",
    "            CSP(512, 512,1),\n",
    "            SPPF(512, 1024)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        out = self.layer2(x)\n",
    "        out2 = self.layer3(out)\n",
    "        out3 = self.layer4(out2)\n",
    "        return out, out2, out3\n",
    "\n",
    "# Neck\n",
    "class PANet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv5_reduce = ConvBlock(1024, 512, 1)\n",
    "        self.conv4_reduce = ConvBlock(256, 512, 1)\n",
    "        self.csp4_td = CSP(1024, 256, num_blocks=1)\n",
    "\n",
    "        self.conv4_reduce2 = ConvBlock(256, 128, 1)\n",
    "        self.conv3_reduce = ConvBlock(128, 128, 1)\n",
    "        self.csp3_td = CSP(256, 128, num_blocks=1)\n",
    "\n",
    "        self.conv3_down = ConvBlock(128, 128, 3, stride=2)\n",
    "        self.csp4_bu = CSP(384, 256, num_blocks=1)\n",
    "\n",
    "        self.conv4_down = ConvBlock(256, 256, 3, stride=2)\n",
    "        self.csp5_bu = CSP(768, 512, num_blocks=1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x3, x4, x5):\n",
    "        p5 = self.conv5_reduce(x5)\n",
    "        p5_up = self.upsample(p5)\n",
    "        p4 = self.conv4_reduce(x4)\n",
    "        p4_td = self.csp4_td(torch.cat([p5_up, p4], dim=1))\n",
    "\n",
    "        p4_red = self.conv4_reduce2(p4_td)\n",
    "        p4_up = self.upsample(p4_red)\n",
    "        p3 = self.conv3_reduce(x3)\n",
    "        p3_td = self.csp3_td(torch.cat([p4_up, p3], dim=1))\n",
    "\n",
    "        p3_down = self.conv3_down(p3_td)\n",
    "        p4_bu = self.csp4_bu(torch.cat([p3_down, p4_td], dim=1))\n",
    "\n",
    "        p4_down = self.conv4_down(p4_bu)\n",
    "        p5_bu = self.csp5_bu(torch.cat([p4_down, p5], dim=1))\n",
    "\n",
    "        return p3_td, p4_bu, p5_bu\n",
    "\n",
    "# Head\n",
    "class HeadDetection(nn.Module):\n",
    "    def __init__(self, in_channels,num_classes,anchor):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchor = anchor\n",
    "        self.conv = ConvBlock(in_channels, in_channels * 2,3)\n",
    "        self.final_conv = nn.Conv2d(in_channels * 2, (num_classes + 5) * anchor, kernel_size=1, stride=1, padding=0)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.final_conv(x)\n",
    "        bs, _, h, w = x.shape\n",
    "        x = x.view(bs, self.anchor, self.num_classes + 5, h, w)\n",
    "        return x.permute(0,1,3,4,2)\n",
    "\n",
    "class YOLOv5(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, anchors = None):\n",
    "        super().__init__()\n",
    "        self.backbone = DarkNet()\n",
    "        self.neck = PANet()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if anchors is None:\n",
    "            self.anchors = torch.tensor([\n",
    "                [[10, 13], [16, 30], [33, 23]],\n",
    "                [[30, 61], [62, 45], [59, 119]],\n",
    "                [[116, 90], [156, 198], [373, 326]]\n",
    "            ], dtype=torch.float32)\n",
    "        else:\n",
    "            self.anchors = anchors\n",
    "        \n",
    "        self.head_small = HeadDetection(128, num_classes, 3)\n",
    "        self.head_medium = HeadDetection(256, num_classes, 3)\n",
    "        self.head_large = HeadDetection(512, num_classes, 3)#Ne supporte pas 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1, out2, out3 = self.backbone(x)\n",
    "        new_out1, new_out2, new_out3 = self.neck(out1, out2, out3)\n",
    "        y_small = self.head_small(new_out1)\n",
    "        y_medium = self.head_medium(new_out2)\n",
    "        y_large = self.head_large(new_out3)\n",
    "        return [y_small, y_medium, y_large]\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLOv5(num_classes=classes_number).to(device)\n",
    "\n",
    "summary(model, (3, 640, 640), device=str(device))\n",
    "\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "params = {name: param for name, param in model.named_parameters() if isinstance(param, torch.Tensor)}\n",
    "\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "y = model(x)\n",
    "\n",
    "make_dot(y[0], params=dict(model.named_parameters())).render(\"yolov5_arch\", format=\"png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv5(num_classes=classes_number).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e450c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(labels, anchors, grid_sizes, num_classes, device):\n",
    "    num_scales = 3\n",
    "    targets = []\n",
    "    for scale_idx in range(num_scales):\n",
    "        grid_size = grid_sizes[scale_idx]\n",
    "        stride = 640 / grid_size\n",
    "        current_anchors = anchors[scale_idx]\n",
    "        target = torch.zeros((len(labels), len(current_anchors), grid_size, grid_size, 6), device=device)\n",
    "        \n",
    "        for batch_idx, label in enumerate(labels):\n",
    "            if label.numel() == 0:\n",
    "                continue\n",
    "            gt_boxes = label[:, 1:5] * 640\n",
    "            gt_classes = label[:, 0].long()\n",
    "            x_center = gt_boxes[:, 0]\n",
    "            y_center = gt_boxes[:, 1]\n",
    "            w = gt_boxes[:, 2]\n",
    "            h = gt_boxes[:, 3]\n",
    "            \n",
    "            for box_idx in range(gt_boxes.shape[0]):\n",
    "                x = x_center[box_idx]\n",
    "                y = y_center[box_idx]\n",
    "                box_w = w[box_idx]\n",
    "                box_h = h[box_idx]\n",
    "                \n",
    "                # Find best anchor\n",
    "                current_anchors_tensor = torch.tensor(current_anchors, device=device)\n",
    "                ratios = torch.cat([box_w / current_anchors_tensor[:, 0],\n",
    "                                  box_h / current_anchors_tensor[:, 1]]).view(2, -1).T\n",
    "                max_ratios = torch.max(ratios, 1/ratios).max(dim=1)[0]\n",
    "                best_anchor = max_ratios.argmin()\n",
    "                \n",
    "                # Grid cell\n",
    "                i = (x / stride).floor().long()\n",
    "                j = (y / stride).floor().long()\n",
    "                if i >= grid_size or j >= grid_size or i < 0 or j < 0:\n",
    "                    continue\n",
    "                \n",
    "                # Target values\n",
    "                tx = (x / stride) - i.float()\n",
    "                ty = (y / stride) - j.float()\n",
    "                tw = torch.log(box_w / current_anchors_tensor[best_anchor, 0] + 1e-16)\n",
    "                th = torch.log(box_h / current_anchors_tensor[best_anchor, 1] + 1e-16)\n",
    "                \n",
    "                target[batch_idx, best_anchor, j, i, 0] = 1.0  # obj\n",
    "                target[batch_idx, best_anchor, j, i, 1] = tx\n",
    "                target[batch_idx, best_anchor, j, i, 2] = ty\n",
    "                target[batch_idx, best_anchor, j, i, 3] = tw\n",
    "                target[batch_idx, best_anchor, j, i, 4] = th\n",
    "                target[batch_idx, best_anchor, j, i, 5] = gt_classes[box_idx]\n",
    "                \n",
    "        targets.append(target)\n",
    "    return targets\n",
    "\n",
    "def compute_loss(outputs, targets):\n",
    "    total_loss = 0.0\n",
    "    obj_loss = 0.0\n",
    "    box_loss = 0.0\n",
    "    cls_loss = 0.0\n",
    "    \n",
    "    for y_pred, target in zip(outputs, targets):\n",
    "        # Predicted components\n",
    "        pred_tx = y_pred[..., 0]\n",
    "        pred_ty = y_pred[..., 1]\n",
    "        pred_tw = y_pred[..., 2]\n",
    "        pred_th = y_pred[..., 3]\n",
    "        pred_obj = y_pred[..., 4]\n",
    "        pred_cls = y_pred[..., 5:]\n",
    "        \n",
    "        # Target components\n",
    "        target_obj = target[..., 0]\n",
    "        target_tx = target[..., 1]\n",
    "        target_ty = target[..., 2]\n",
    "        target_tw = target[..., 3]\n",
    "        target_th = target[..., 4]\n",
    "        target_cls = target[..., 5].long()\n",
    "        \n",
    "        # Objectness loss\n",
    "        obj_mask = target_obj.bool()\n",
    "        obj_loss += F.binary_cross_entropy_with_logits(pred_obj, target_obj)\n",
    "        \n",
    "        # Box loss (only where objects exist)\n",
    "        if obj_mask.sum():\n",
    "            box_loss += F.mse_loss(pred_tx[obj_mask], target_tx[obj_mask]) + \\\n",
    "                       F.mse_loss(pred_ty[obj_mask], target_ty[obj_mask]) + \\\n",
    "                       F.mse_loss(pred_tw[obj_mask], target_tw[obj_mask]) + \\\n",
    "                       F.mse_loss(pred_th[obj_mask], target_th[obj_mask])\n",
    "        \n",
    "        # Class loss\n",
    "        if obj_mask.sum():\n",
    "            cls_loss += F.cross_entropy(pred_cls[obj_mask], target_cls[obj_mask])\n",
    "    \n",
    "    total_loss = obj_loss + box_loss + cls_loss\n",
    "    return total_loss, obj_loss, box_loss, cls_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73677f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import wandb\n",
    "print(f\"Check that the following key is your key on wandb {wandb.api.api_key}\")\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"s-gardier-work\",\n",
    "    project=\"yolov5\",\n",
    "    config={\n",
    "        \"architecture\": \"YOLOv5\",\n",
    "        \"dataset\": \"https://universe.roboflow.com/yolo-minecraft/minecraft-ogpjp/dataset/10\",\n",
    "        \"epochs\": num_epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLOv5(num_classes=classes_number).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "        \n",
    "        # Generate targets\n",
    "        anchors = model.anchors.cpu().numpy().tolist()\n",
    "        grid_sizes = [80, 40, 20]\n",
    "        targets = build_targets(labels, anchors, grid_sizes, classes_number, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, obj_loss, box_loss, cls_loss = compute_loss(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}\")\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    run.log({\"loss\":epoch_loss})\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"yolov5_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Model saved at epoch {epoch + 1}\")\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(validloader):\n",
    "        images = images.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "        \n",
    "        # Generate targets\n",
    "        anchors = model.anchors.cpu().numpy().tolist()\n",
    "        grid_sizes = [80, 40, 20]\n",
    "        targets = build_targets(labels, anchors, grid_sizes, classes_number, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, obj_loss, box_loss, cls_loss = compute_loss(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        val_running_loss += total_loss.item()\n",
    "    print(f\"Validation Loss: {val_running_loss / len(validloader):.4f}\")\n",
    "\n",
    "run.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(outputs, confidence_thresh=0.1, iou_thresh=0.1):\n",
    "    \"\"\"Convert model outputs to usable bounding boxes\"\"\"\n",
    "    all_boxes = []\n",
    "    anchors = model.anchors.cpu().numpy()\n",
    "    strides = [8, 16, 32]  # For 640x640 input\n",
    "    \n",
    "    for scale_idx, output in enumerate(outputs):\n",
    "        # Convert to numpy and get dimensions\n",
    "        output = output.sigmoid().cpu().detach().numpy()\n",
    "        bs, num_anchors, h, w, _ = output.shape\n",
    "        \n",
    "        # Convert dimensions to integers\n",
    "        h = int(h)\n",
    "        w = int(w)\n",
    "        \n",
    "        # Get parameters for this scale\n",
    "        stride = strides[scale_idx]\n",
    "        anchor = anchors[scale_idx]\n",
    "        \n",
    "        # Create grid\n",
    "        grid_y, grid_x = np.mgrid[:h, :w]\n",
    "\n",
    "        # Reshape output for vectorized operations\n",
    "        output = output.reshape(bs, num_anchors, h, w, -1)\n",
    "        \n",
    "        # Decode predictions using vectorized operations\n",
    "        tx = output[..., 0]\n",
    "        ty = output[..., 1]\n",
    "        tw = output[..., 2]\n",
    "        th = output[..., 3]\n",
    "        obj = output[..., 4]\n",
    "        cls_probs = output[..., 5:]\n",
    "        \n",
    "        # Calculate absolute coordinates\n",
    "        x = (grid_x + tx) * stride\n",
    "        y = (grid_y + ty) * stride\n",
    "        w = anchor[:, 0].reshape(1, -1, 1, 1) * np.exp(tw)\n",
    "        h = anchor[:, 1].reshape(1, -1, 1, 1) * np.exp(th)\n",
    "        \n",
    "        # Calculate class confidence\n",
    "        class_ids = np.argmax(cls_probs, axis=-1)\n",
    "        class_conf = np.take_along_axis(cls_probs, class_ids[..., None], axis=-1).squeeze(-1)\n",
    "        confidence = obj * class_conf\n",
    "\n",
    "        # Filter by confidence threshold\n",
    "        mask = confidence > confidence_thresh\n",
    "        for batch_idx in range(bs):\n",
    "            batch_mask = mask[batch_idx]\n",
    "            batch_boxes = np.stack([\n",
    "                x[batch_idx][batch_mask] - w[batch_idx][batch_mask]/2,\n",
    "                y[batch_idx][batch_mask] - h[batch_idx][batch_mask]/2,\n",
    "                x[batch_idx][batch_mask] + w[batch_idx][batch_mask]/2,\n",
    "                y[batch_idx][batch_mask] + h[batch_idx][batch_mask]/2,\n",
    "                confidence[batch_idx][batch_mask],\n",
    "                class_ids[batch_idx][batch_mask]\n",
    "            ], axis=-1)\n",
    "            \n",
    "            if batch_boxes.size > 0:\n",
    "                all_boxes.extend(batch_boxes.tolist())\n",
    "\n",
    "    # Non-Maximum Suppression\n",
    "    if not all_boxes:\n",
    "        return []\n",
    "\n",
    "    boxes = np.array(all_boxes)\n",
    "    x1, y1, x2, y2, scores, class_ids = boxes.T\n",
    "\n",
    "    # Calculate areas and sort\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        \n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        intersection = w * h\n",
    "        \n",
    "        iou = intersection / (areas[i] + areas[order[1:]] - intersection)\n",
    "        \n",
    "        # Filter boxes\n",
    "        inds = np.where(iou <= iou_thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return boxes[keep].tolist()\n",
    "\n",
    "def test_single_image(model, dataset, index=0):\n",
    "    # Get image and labels\n",
    "    image, true_labels = dataset[index]\n",
    "    true_labels = true_labels.cpu().numpy()\n",
    "    \n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image.unsqueeze(0).to(device))\n",
    "    \n",
    "    # Decode predictions\n",
    "    pred_boxes = decode_predictions(outputs)\n",
    "    \n",
    "    # Convert true labels to box format\n",
    "    true_boxes = []\n",
    "    img_w, img_h = 640, 640  # Our image size\n",
    "    for label in true_labels:\n",
    "        class_id, xc, yc, bw, bh = label\n",
    "        x = (xc - bw/2) * img_w\n",
    "        y = (yc - bh/2) * img_h\n",
    "        w = bw * img_w\n",
    "        h = bh * img_h\n",
    "        true_boxes.append([x, y, x+w, y+h, 1.0, class_id])\n",
    "    \n",
    "    # Visualize\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image_np)\n",
    "    \n",
    "    # Draw true boxes (green)\n",
    "    '''\n",
    "    for box in true_boxes:\n",
    "        x1, y1, x2, y2, _, class_id = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='lime', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, classes_types[int(class_id)],\n",
    "                color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='lime', alpha=0.8, pad=1))\n",
    "    '''\n",
    "    \n",
    "    # Draw predicted boxes (red)\n",
    "    for box in pred_boxes:\n",
    "        x1, y1, x2, y2, conf, class_id = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, f\"{classes_types[int(class_id)]} {conf:.2f}\",\n",
    "                color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='red', alpha=1, pad=1))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test on first validation image\n",
    "test_single_image(model, mc_valid, index=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info8010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
